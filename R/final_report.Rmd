---
title: "MTHM053 Applications of Data Science and Statistics – Coursework"
author: "Duc Anh Tuan, Nguyen -  740073529"
output: pdf_document
---

# Student declaration
This assessment is AI-supported. I acknowledge the following uses of GenAI tools in this assessment.

[x] I have used GenAI tools to suggest section headings for my report.

[x] I have used GenAI tools to help me to correct my grammar or spelling.

[x] I have used GenAI tools to suggest topics to discuss in my literature review.

[x] I declare that I have referenced the use of GenAI outputs within my assessment, in line with the University’s referencing guidelines.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      include = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      cache = TRUE)
# Load packages
library(tidyverse)
library(tidymodels)
library(here)
library(DBI)
library(RPostgres)
# Load load_data function
source(here("R", "load_data.R"))
```

# Github repository
This report is submitted together with my Github repository at (https://github.com/toshi2135/mthm503-app-data-science)

# Task 01 - Supervised Classification Task

## Introduction

Pedestrian injuries in road traffic collisions represent a critical public health concern, with policy implications for urban design, traffic enforcement, and public safety. Accurate prediction of injury severity can support targeted interventions and triage efforts. This task focuses on predicting the severity of pedestrian casualties using the UK STATS19 dataset, applying supervised machine learning models to assess how demographic, environmental, and contextual factors relate to injury outcomes. The objective is twofold: to evaluate the comparative performance of classification models and to interpret the contribution of individual predictors in a high-stakes, imbalanced setting.

## Data Preprocessing and Class Balance
The dataset was constructed from PostgreSQL tables by filtering for pedestrian casualties and joining accident and vehicle information. After cleaning and de-duplication, 23,850 complete cases remained. Categorical variables were encoded as factors, and missing values were imputed using median (numeric) or mode/“Missing” (categorical). This imputation approach was chosen for its robustness to outliers and minimal bias under Missing at Random (MAR) assumptions, though sensitivity analysis remains an area for future refinement.

The outcome variable `casualty_severity` was highly imbalanced, with approximately 73% of cases classified as Slight, 24% as Serious, and only 3% as Fatal. This class skew necessitated careful evaluation of metrics beyond accuracy—particularly precision, recall, and F1-score per class—to ensure that models did not disproportionately favour the dominant class.

```{r task01_data_query} 
# Data querying
## Use the .Renviron file to set the environment variables and connect to DB
## Read .Renviron file
readRenviron(".Renviron")
## Check if the environment variables are set
Sys.getenv("PGRHOST")
## Connect to the database
conn <- get_db_connection()
## Check the connection
DBI::dbIsValid(conn)
## Check the tables in the database
tables <- DBI::dbListTables(conn)
tables
## Check the first few rows of the casualty_pedestrian table
casualty_pedestrian <- DBI::dbReadTable(conn, "stats19_casualties")
names(casualty_pedestrian)
glimpse(casualty_pedestrian)
## Check the first few rows of the accident table
accident <- DBI::dbReadTable(conn, "stats19_accidents")
names(accident)
glimpse(accident)
## Check the first few rows of the vehicle table
vehicle <- DBI::dbReadTable(conn, "stats19_vehicles")
names(vehicle)
glimpse(vehicle)
## Read SQL query to join the tables
sql_query <- readLines(here("sql", "01_sup_data_query.sql"))
query <- paste(sql_query, collapse = "\n")
sup_data <- dbGetQuery(conn, query)
## Check the data
names(sup_data)
glimpse(sup_data)
summary(sup_data)
## Close the connection
DBI::dbDisconnect(conn)
```

```{r data_preprocessing}
# Data Preprocessing
## Drop `obs_date` column
sup_data <- sup_data %>% select(-obs_date)
## Ensure the target variable is a factor
sup_data$casualty_severity <- as.factor(sup_data$casualty_severity)
## Check the distribution of the target variable
sup_data %>%
  count(casualty_severity) %>%
  mutate(prop = n / sum(n))
## Check for missing values
sup_data %>% skimr::skim()
colSums(is.na(sup_data))
sup_data %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "missing_count"
  ) %>%
  filter(missing_count > 0) %>%
  arrange(desc(missing_count))
## Impute missing data
### Impute NA features
library(forcats)
library(dplyr)
sup_data <- sup_data %>%
  mutate(
    vehicle_manoeuvre = fct_na_value_to_level(
      vehicle_manoeuvre,
      level = "Missing"
    ),
    driver_home_area_type = fct_na_value_to_level(
      driver_home_area_type,
      level = "Missing"
    ),
    casualty_home_area_type = fct_na_value_to_level(
      casualty_home_area_type,
      level = "Missing"
    ),
    pedestrian_crossing_physical_facilities = fct_na_value_to_level(
      pedestrian_crossing_physical_facilities,
      level = "Missing"
    ),
    carriageway_hazards = fct_na_value_to_level(
      carriageway_hazards,
      level = "Missing"
    ),
    road_surface_conditions = fct_na_value_to_level(
      road_surface_conditions,
      level = "Missing"
    ),
    junction_detail = fct_na_value_to_level(junction_detail, level = "Missing"),
    sex_of_casualty = fct_na_value_to_level(sex_of_casualty, level = "Missing")
  )
### Impute `age_of_vehicle` with median
median_age_vehicle <- median(sup_data$age_of_vehicle, na.rm = TRUE)
sup_data <- sup_data %>%
  mutate(
    age_of_vehicle = if_else(
      is.na(age_of_vehicle),
      median_age_vehicle,
      age_of_vehicle
    )
  )
## Check the data again
glimpse(sup_data)
summary(sup_data)
sup_data %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "var", values_to = "missing") %>%
  filter(missing > 0)
## Encode categorical variables to factors
sup_data <- sup_data %>%
  mutate(
    pedestrian_movement = as.factor(pedestrian_movement),
    weather_conditions = as.factor(weather_conditions),
    light_conditions = as.factor(light_conditions),
    urban_or_rural_area = as.factor(urban_or_rural_area),
    road_type = as.factor(road_type),
    sex_of_driver = as.factor(sex_of_driver),
    journey_purpose_of_driver = as.factor(journey_purpose_of_driver),
    vehicle_type = as.factor(vehicle_type),
    hour_of_day = as.factor(hour_of_day),
    day_of_week = as.factor(day_of_week),
    is_weekend = as.factor(is_weekend)
  )
sup_data <- sup_data %>%
  mutate(
    day_of_week = factor(
      day_of_week,
      levels = 0:6,
      labels = c(
        "Sunday",
        "Monday",
        "Tuesday",
        "Wednesday",
        "Thursday",
        "Friday",
        "Saturday"
      )
    ),
    is_weekend = factor(
      is_weekend,
      levels = c(0, 1),
      labels = c("Weekday", "Weekend")
    ),
    casualty_type = factor(casualty_type),
    vehicle_type = factor(vehicle_type)
  )
## Check the data again
glimpse(sup_data)
sup_data %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "var", values_to = "missing") %>%
  filter(missing > 0)
## Remove `accident_index` column
sup_data <- sup_data %>% select(-accident_index)
## Remove `casualty_type` column
sup_data <- sup_data %>% select(-casualty_type)
## Split the data for train and test
split <- initial_split(sup_data, strata = casualty_severity)
train_data <- training(split)
count(train_data)
test_data <- testing(split)
count(test_data)
```

## Modelling Framework and Rationale

Four classification models were constructed using the `tidymodels` framework. A multinomial logistic regression (MLR) was selected as a baseline due to its parametric interpretability and widespread use in public health research. A Random Forest (RF) model served as the primary non-parametric alternative, offering the ability to capture non-linear interactions and rank feature importance. To address class imbalance, an RF model incorporating inverse frequency class weights was trained, avoiding the need for synthetic oversampling. Finally, a tuned RF model explored hyperparameter optimisation over `mtry`, `min_n`, and `trees`, selected via random grid search with 5-fold cross-validation.

The rationale for model selection was grounded in the trade-off between bias and variance: while MLR offers interpretability and fast training, RF provides flexible modelling capacity and robustness to overfitting, especially in the presence of complex feature interactions. The use of class weights aligns with literature advocating cost-sensitive learning in imbalanced health datasets (He and Garcia, 2009).

```{r baseline_rf}
# Build Random Forest baseline model
library(ranger)
## Build the recipe
rf_rec <- recipe(casualty_severity ~ ., data = train_data)
## Build the model specification
rf_spec <- rand_forest(trees = 500) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
## Build the workflow
rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_spec)
## Fit the model
rf_fit <- rf_wf %>%
  fit(data = train_data)
```

```{r baseline_rf_eval, include=TRUE}
# Evaluate the model
## Check the model
rf_preds <- predict(rf_fit, test_data, type = "prob") %>%
  bind_cols(predict(rf_fit, test_data)) %>%
  bind_cols(test_data)
```

```{r baseline_rf_eval_roc, echo=FALSE}
## Check the ROC curve
roc_curve(
  rf_preds,
  truth = casualty_severity,
  .pred_Slight,
  .pred_Serious,
  .pred_Fatal
) %>%
  autoplot() +
  labs(
    title = "ROC Curve for Random Forest Model",
    x = "False Positive Rate",
    y = "True Positive Rate"
  ) +
  theme_minimal()
```

```{r baseline_rf_eval_confmat, echo=FALSE}
## Check the confusion matrix
conf_mat(rf_preds, truth = casualty_severity, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Confusion Matrix for Random Forest Model",
    x = "Predicted",
    y = "Actual"
  ) +
  theme_minimal()
```

```{r baseline_rf_eval_metrics, echo=FALSE}
## Check the accuracy, precision, recall, and F1 score
rf_accuracy <- rf_preds %>%
  accuracy(truth = casualty_severity, estimate = .pred_class)
rf_precision <- rf_preds %>%
  precision(truth = casualty_severity, estimate = .pred_class)
rf_recall <- rf_preds %>%
  recall(truth = casualty_severity, estimate = .pred_class)
rf_f1 <- rf_preds %>%
  f_meas(truth = casualty_severity, estimate = .pred_class)
```

```{r baseline_rf_eval_importance}
## Find the importance of the features
vip::vip(rf_fit, num_features = 10) +
  labs(title = "Feature Importance for Random Forest Model") +
  theme_minimal()
```

```{r baseline_rf_eval_summary, echo=FALSE}
## Create a summary table and show it
rf_summary <- tibble(
  model = "Random Forest",
  accuracy = rf_accuracy$.estimate,
  precision = rf_precision$.estimate,
  recall = rf_recall$.estimate,
  f1_score = rf_f1$.estimate
)
rf_summary %>%
  knitr::kable(caption = "Random Forest Model Summary")
```

```{r baseline_mlr}
# Build Logistic Regression baseline model
library(nnet)
## Build the recipe
log_rec <- recipe(casualty_severity ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())
## Build the model specification
log_spec <- multinom_reg(penalty = 0) %>%
  set_engine("nnet") %>%
  set_mode("classification")
## Build the workflow
log_wf <- workflow() %>%
  add_recipe(log_rec) %>%
  add_model(log_spec)
## Fit the model
log_fit <- log_wf %>%
  fit(data = train_data)
```


```{r baseline_mlr_eval}
# Evaluate the model
## Check the model
log_preds <- predict(log_fit, test_data, type = "prob") %>%
  bind_cols(predict(log_fit, test_data)) %>%
  bind_cols(test_data)
```

```{r baseline_mlr_eval_roc, echo=FALSE}
## Check the ROC curve
roc_curve(
  log_preds,
  truth = casualty_severity,
  .pred_Slight,
  .pred_Serious,
  .pred_Fatal
) %>%
  autoplot() +
  labs(
    title = "ROC Curve for Logistic Regression Model",
    x = "False Positive Rate",
    y = "True Positive Rate"
  ) +
  theme_minimal()
```

```{r baseline_mlr_eval_confmat, echo=FALSE}
## Check the confusion matrix
conf_mat(log_preds, truth = casualty_severity, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Confusion Matrix for Logistic Regression Model",
    x = "Predicted",
    y = "Actual"
  ) +
  theme_minimal()
```

```{r baseline_mlr_eval_metrics, echo=FALSE}
## Check the accuracy, precision, recall, and F1 score
log_accuracy <- log_preds %>%
  accuracy(truth = casualty_severity, estimate = .pred_class)
log_precision <- log_preds %>%
  precision(truth = casualty_severity, estimate = .pred_class)
log_recall <- log_preds %>%
  recall(truth = casualty_severity, estimate = .pred_class)
log_f1 <- log_preds %>%
  f_meas(truth = casualty_severity, estimate = .pred_class)
```

```{r baseline_mlr_eval_importance}
## Create a summary table and show it
log_summary <- tibble(
  model = "Logistic Regression",
  accuracy = log_accuracy$.estimate,
  precision = log_precision$.estimate,
  recall = log_recall$.estimate,
  f1_score = log_f1$.estimate
)
log_summary %>%
  knitr::kable(caption = "Logistic Regression Model Summary")
```

## Interpretation of Feature Importance and Error Patterns
The Random Forest model revealed several dominant predictors. `age_of_casualty` emerged as the most influential variable, aligning with existing research highlighting increased vulnerability among older adults and children (Oxley et al., 2004). Temporal features such as `hour_of_day` and `day_of_week` were also ranked highly, reflecting diurnal and weekday/weekend exposure patterns. Environmental conditions, including `road_surface_conditions` and `urban_or_rural_area`, contributed meaningfully but less dominantly, suggesting contextual influence without being primary determinants.

A key insight emerged from confusion matrix analysis: while recall for the Slight class was relatively high, the model systematically confused Serious and Slight categories. This ambiguity likely reflects overlapping contextual features—such as moderate-speed urban collisions—that result in non-distinct injury outcomes. Importantly, the Fatal class remained difficult to predict, with recall hovering below 0.30 in all models, underscoring the limitations of rare-event prediction using imbalanced observational data.

```{r rf_case_weights}
# Apply case weights to the Random Forest model
## Calculate case weights based on the target variable distribution
class_weights <- sup_data %>%
  count(casualty_severity) %>%
  mutate(weight = 1 / n) %>%
  select(casualty_severity, weight)
## Join the case weights to the training data
sup_data_weighted <- sup_data %>%
  left_join(class_weights, by = "casualty_severity")
## Split the weighted data for train and test
split_weighted <- initial_split(sup_data_weighted, strata = casualty_severity)
train_data_weighted <- training(split_weighted)
count(train_data_weighted)
test_data_weighted <- testing(split_weighted)
count(test_data_weighted)
## Build the recipe
rf_rec_weighted <- recipe(casualty_severity ~ ., data = train_data_weighted) %>%
  update_role(weight, new_role = "case_weight")
## Build the model specification
rf_spec_weighted <- rand_forest(trees = 500) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
## Build the workflow
rf_wf_weighted <- workflow() %>%
  add_recipe(rf_rec_weighted) %>%
  add_model(rf_spec_weighted)
## Fit the model with case weights
rf_fit_weighted <- rf_wf_weighted %>%
  fit(data = train_data_weighted)
```

```{r rf_case_weights_eval}
# Evaluate the Random Forest model with case weights
## Check the model
rf_preds_weighted <- predict(
  rf_fit_weighted,
  test_data_weighted,
  type = "prob"
) %>%
  bind_cols(predict(rf_fit_weighted, test_data_weighted)) %>%
  bind_cols(test_data_weighted)
## Check the ROC curve
roc_curve(
  rf_preds_weighted,
  truth = casualty_severity,
  .pred_Slight,
  .pred_Serious,
  .pred_Fatal
) %>%
  autoplot() +
  labs(
    title = "ROC Curve for Random Forest Model with Case Weights",
    x = "False Positive Rate",
    y = "True Positive Rate"
  ) +
  theme_minimal()
## Check the confusion matrix
conf_mat(
  rf_preds_weighted,
  truth = casualty_severity,
  estimate = .pred_class
) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Confusion Matrix for Random Forest Model with Case Weights",
    x = "Predicted",
    y = "Actual"
  ) +
  theme_minimal()
## Check the accuracy, precision, recall, and F1 score
rf_accuracy_weighted <- rf_preds_weighted %>%
  accuracy(truth = casualty_severity, estimate = .pred_class)
rf_precision_weighted <- rf_preds_weighted %>%
  precision(truth = casualty_severity, estimate = .pred_class)
rf_recall_weighted <- rf_preds_weighted %>%
  recall(truth = casualty_severity, estimate = .pred_class)
rf_f1_weighted <- rf_preds_weighted %>%
  f_meas(truth = casualty_severity, estimate = .pred_class)
## Create a summary table and show it
rf_summary_weighted <- tibble(
  model = "Random Forest with Case Weights",
  accuracy = rf_accuracy_weighted$.estimate,
  precision = rf_precision_weighted$.estimate,
  recall = rf_recall_weighted$.estimate,
  f1_score = rf_f1_weighted$.estimate
)
rf_summary_weighted %>%
  knitr::kable(caption = "Random Forest Model with Case Weights Summary")
```

```{r rf_tuning}
# Hyperparameter tuning for Random Forest model
library(tune)
## Define the grid for hyperparameter tuning
rf_grid <- grid_random(
  mtry(range = c(3, 15)),
  min_n(range = c(2, 20)),
  trees(range = c(100, 1000)),
  size = 20
)
## Build the specification for hyperparameter tuning
rf_spec_tune <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune(),
) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")
## Build the recipe
rf_rec <- recipe(casualty_severity ~ ., data = train_data)
## Build the workflow for hyperparameter tuning
rf_wf_tune <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_spec_tune)
## Perform hyperparameter tuning using cross-validation
set.seed(123)
rf_tune_results <- rf_wf_tune %>%
  tune_grid(
    resamples = vfold_cv(train_data, v = 5, strata = casualty_severity),
    grid = rf_grid,
    metrics = metric_set(accuracy, f_meas, roc_auc, precision, recall)
  )
```

```{r rf_tuning_eval}
## Check the tuning results
rf_tune_results %>%
  collect_metrics() %>%
  arrange(desc(mean)) %>%
  knitr::kable(
    caption = "Hyperparameter Tuning Results for Random Forest Model"
  )
```

```{r rf_tuning_best}
## Select the best hyperparameters
collect_metrics(rf_tune_results) %>% distinct(.metric)
best_rf_params <- select_best(rf_tune_results, metric = "f_meas")
best_rf_params # (mtry=3, trees=855, min_n=8)
## Finalize the workflow with the best hyperparameters
rf_wf_final <- rf_wf_tune %>%
  finalize_workflow(best_rf_params)
## Fit the final model with the best hyperparameters
rf_fit_final <- rf_wf_final %>%
  fit(data = train_data)
```

```{r rf_tuning_final_eval}
# Evaluate the tuned Random Forest model
## Check the final model
rf_preds_final <- predict(rf_fit_final, test_data, type = "prob") %>%
  bind_cols(predict(rf_fit_final, test_data)) %>%
  bind_cols(test_data)
```

```{r rf_tuning_final_eval_roc, include=TRUE, fig.height=3}
## Check the ROC curve for the final model
roc_curve(
  rf_preds_final,
  truth = casualty_severity,
  .pred_Slight,
  .pred_Serious,
  .pred_Fatal
) %>%
  autoplot() +
  labs(
    title = "ROC Curve for Final Random Forest Model",
    x = "False Positive Rate",
    y = "True Positive Rate"
  ) +
  theme_minimal()
```

```{r rf_tuning_final_eval_confmat, include=TRUE, fig.height=3}
## Check the confusion matrix for the final model
conf_mat(rf_preds_final, truth = casualty_severity, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Confusion Matrix for Final Random Forest Model",
    x = "Predicted",
    y = "Actual"
  ) +
  theme_minimal()
```

```{r rf_tuning_final_eval_metrics}
## Check the accuracy, precision, recall, and F1 score for the final model
rf_accuracy_final <- rf_preds_final %>%
  accuracy(truth = casualty_severity, estimate = .pred_class)
rf_precision_final <- rf_preds_final %>%
  precision(truth = casualty_severity, estimate = .pred_class)
rf_recall_final <- rf_preds_final %>%
  recall(truth = casualty_severity, estimate = .pred_class)
rf_f1_final <- rf_preds_final %>%
  f_meas(truth = casualty_severity, estimate = .pred_class)
## Create a summary table for the final model
rf_summary_final <- tibble(
  model = "Final Random Forest",
  accuracy = rf_accuracy_final$.estimate,
  precision = rf_precision_final$.estimate,
  recall = rf_recall_final$.estimate,
  f1_score = rf_f1_final$.estimate
)
rf_summary_final %>%
  knitr::kable(caption = "Final Random Forest Model Summary")
```

## Model Performance and Evaluation
```{r results_comparison, include=TRUE}
# Create a summary table with all models
sup_summary <- bind_rows(
  rf_summary,
  log_summary,
  rf_summary_weighted,
  rf_summary_final
) %>%
  mutate(
    model = factor(
      model,
      levels = c(
        "Random Forest",
        "Logistic Regression",
        "Random Forest with Case Weights",
        "Final Random Forest"
      )
    )
  ) %>%
  knitr::kable(
    caption = "Model Comparison Summary"
  )
sup_summary
```

The weighted RF model achieved the highest macro-averaged F1-score (0.578), outperforming both the baseline RF and MLR models. Notably, hyperparameter tuning of the RF provided negligible performance gain, suggesting that the out-of-the-box configuration was close to optimal. ROC AUC for the weighted RF reached 0.74 (hand-till method), demonstrating good class separability despite the imbalance.

Evaluation focused on macro-F1, precision-recall trade-offs, and confusion matrices, avoiding reliance on overall accuracy due to its insensitivity in skewed distributions. The superior performance of the weighted RF model supports the use of cost-sensitive ensemble methods in safety-critical classification tasks.

## Conclusion and Implications

The results underscore the efficacy of Random Forest classifiers, particularly when combined with class weighting, for modelling pedestrian injury severity in imbalanced datasets. While logistic regression offers transparency, it falls short in capturing complex interactions that influence rare outcomes. Future work should explore calibration metrics (e.g., Brier score), post-hoc explainability techniques such as SHAP values, and incorporation of additional variables (e.g., vehicle impact speed, lighting conditions) that may improve predictive resolution, especially for the Fatal class. Ultimately, predictive models in this context must balance performance with interpretability to be actionable for urban planners and public health stakeholders.


---

# Task 02 – Regression Analysis Task

## Objective

This task investigates whether demographic variables, specifically age and sex, influence the type of extrication technique used by the Fire and Rescue Service in road traffic incidents. The study aims to determine whether these human attributes are associated with operational decisions involving one of six specialist extrication procedures, such as roof removal or side extraction. Understanding these relationships is important for ensuring equitable emergency response protocols and avoiding unintended biases in rescue operations.

```{r task02_load_data}
# Data querying
## Use the .Renviron file to set the environment variables and connect to DB
## Read .Renviron file
readRenviron(here(".Renviron"))
## Check if the environment variables are set
Sys.getenv("PGRHOST")
## Connect to the database using the load_data function
conn <- get_db_connection()
## Check the tables in the database
tables <- DBI::dbListTables(conn)
tables
## Check first few rows of fire_rescue_extrication_casualties table
fire_rescue <- DBI::dbReadTable(conn, "fire_rescue_extrication_casualties")
## Check the structure of the data
str(fire_rescue)
## Check the column names
colnames(fire_rescue)
## Check the first few rows
head(fire_rescue)
## Check stats19_by_financial_year table
stats19_by_financial_year <- DBI::dbReadTable(conn, "stats19_by_financial_year")
## Check the structure of the data
str(stats19_by_financial_year)
## Check the column names
colnames(stats19_by_financial_year)
## Read SQL query to join the table
sql_query <- readLines(here("sql", "02_reg_data_query.sql"))
query <- paste(sql_query, collapse = "\n")
fire_rescue_data <- DBI::dbGetQuery(conn, query)
## Check the structure of the data
str(fire_rescue_data)
## Close the connection
DBI::dbDisconnect(conn)
```

## Data Description and Preparation

```{r data_preparation}
# Data preprocessing
library(dplyr)
fire_rescue_clean <- fire_rescue_data %>%
  filter(
    !is.na(extrication),
    !is.na(sex),
    !is.na(age_band),
    !is.na(extrication_rate),
    extrication != "Unknown"
  ) %>%
  mutate(
    extrication = factor(extrication),
    sex = factor(sex),
    age_band = factor(
      age_band,
      levels = c("0-16", "17-24", "25-39", "40-64", "65-74", "75+"),
      ordered = TRUE
    )
  )
str(fire_rescue_clean)
## Check the levels of the factors
levels(fire_rescue_clean$extrication)
levels(fire_rescue_clean$sex)
levels(fire_rescue_clean$age_band)
## Remove rows with Unknown value in sex
fire_rescue_clean <- fire_rescue_clean %>%
  filter(sex != "Unknown") %>%
  droplevels()
## Remove rows with NA value in age_band
fire_rescue_clean <- fire_rescue_clean %>%
  filter(!is.na(age_band)) %>%
  droplevels()
## Check the structure of the cleaned data
str(fire_rescue_clean)
```

The dataset was derived from the `fire_rescue_extrications_casualties` table. Records with missing or unknown values in either `sex` or `age_band` were excluded to ensure clean stratification across the predictors. The final dataset comprised 1,440 aggregated observations, where the response variable `extrication` was a six-level unordered categorical factor representing the type of procedure applied. Predictor variables included `age_band`, `sex`, and their interaction. To avoid misleading polynomial contrast effects, `age_band` was treated as a nominal factor.

Although the dataset captures relevant demographic information, its aggregated nature—rather than individual-level entries—limits inferential resolution and may attenuate subtle associations.

```{r data_summary, include=TRUE, fig.height=3}
## Plot the data
library(ggplot2)
ggplot(fire_rescue_clean, aes(x = age_band, fill = extrication)) +
  geom_bar(position = "fill") +
  labs(
    title = "Extrication Method by Age Band",
    x = "Age Band",
    y = "Proportion",
    fill = "Extrication Method"
  ) +
  theme_minimal()
```

## Primary Modelling: Multinomial Logistic Regression

```{r multinomial_glm}
# Statistical analysis using Multinomial GLM
library(nnet)
## Fit the Multinomial GLM
model_multi <- nnet::multinom(
  extrication ~ age_band + sex + age_band:sex,
  data = fire_rescue_clean
)
```

```{r model_summary, include=TRUE}
## Check the summary of the model
summary(model_multi)
library(broom)
## Tidy the model output
tidy(model_multi, conf.int = TRUE, exponentiate = TRUE)
```

```{r model_coefficients}
## Check the model coefficients
coef(model_multi)
```

```{r model_residuals}
## Check the model residuals
residuals(model_multi)
```

A multinomial logistic regression model was fitted using the `nnet::multinom` function to assess the association between demographic factors and extrication type. The model's structure assumes a logit link for each outcome category relative to a baseline, estimating odds ratios for each level of `age_band`, `sex`, and their interaction. Model diagnostics showed convergence, and coefficient estimates were stable across refittings.

However, no predictor reached statistical significance at the 5% level, and all odds ratios were close to one. Confidence intervals consistently spanned the null value, suggesting a lack of evidence for demographic influence. The model's Akaike Information Criterion (AIC) was 5240.3, and alternative specifications (e.g., with `age_band` recoded or interaction terms removed) yielded identical conclusions, implying robustness across contrast structures.

```{r upgraded_multinomial_glm}
# Upgrade the model, use age_band as nominal factor instead of ordered factor
## Change age_band to nominal factor
fire_rescue_clean$age_band <- factor(
  fire_rescue_clean$age_band,
  ordered = FALSE
)
## Fit the Multinomial GLM again
library(nnet)
model_multi2 <- nnet::multinom(
  extrication ~ age_band + sex + age_band:sex,
  data = fire_rescue_clean
)
## Check the summary of the upgraded model
summary(model_multi2)
## Tidy the upgraded model output
tidy(model_multi2, conf.int = TRUE, exponentiate = TRUE)
## Check the model coefficients
coef(model_multi2)
## Check the model residuals
residuals(model_multi2)
## Compare the models
AIC(model_multi)
AIC(model_multi2)
```

## Supplementary Modelling: Poisson Regression on Casualty Count

Although not central to the primary question, a Poisson regression was conducted to assess whether the number of casualties involved in an incident—a proxy for incident severity—was associated with age, sex, or extrication type. This supplementary analysis provided a different perspective, exploring whether demographics might correlate with the scale of the rescue operation, even if not the specific method chosen.

```{r poisson_model, include=TRUE}
# Apply Poisson Regression Model
model_poisson <- glm(
  n_casualties ~ age_band + sex + extrication,
  data = fire_rescue_clean,
  family = poisson()
)
summary(model_poisson)
```

In contrast to the multinomial model, the Poisson regression identified statistically significant associations (p < 0.001) for all predictors. Male casualties and older age bands were associated with higher casualty counts, and certain extrication types (e.g., roof removal) were also linked to more severe incidents. The model’s AIC was approximately 28,419, and residual deviance indicated a good fit. While these results are not directly applicable to the operational choice of extrication, they suggest that demographics may correlate with incident complexity and rescue burden.


## Diagnostic Evaluation and Independence Testing

```{r chi_squared_tests}
# Model diagnostics with Chi-squared test
table_age <- table(fire_rescue_clean$age_band, fire_rescue_clean$extrication)
chisq.test(table_age)
table_sex <- table(fire_rescue_clean$sex, fire_rescue_clean$extrication)
chisq.test(table_sex)
# Chi-squared test to examine the association between age_band and extrication
chisq_test_result <- chisq.test(table(
  fire_rescue_clean$age_band,
  fire_rescue_clean$extrication
))
```

```{r chisq_test_summary, include=TRUE}
# Print the results of the Chi-squared test
print(chisq_test_result)
```

To corroborate the regression findings, Pearson's chi-squared tests were performed to assess the independence between demographic variables and extrication type. All tests returned p-values equal to 1. While this superficially supports the null hypothesis, such extreme results raise concerns about test validity. It is plausible that low cell counts across the six extrication methods—particularly when stratified by age and sex—violated the assumptions of the chi-squared test. This underscores a broader limitation of using aggregated data in sparse contingency tables.

## Interpretation and Limitations 

The findings provide no evidence that casualty demographics directly influence the choice of extrication method. This outcome is reassuring from an operational ethics perspective, suggesting that rescue decisions are driven by situational factors rather than individual characteristics. However, the results must be interpreted cautiously. First, the use of aggregated records reduces statistical power and obscures within-group heterogeneity. Second, potential confounders—such as vehicle type, entrapment mechanism, or severity of injury—were not included, and their omission may mask true underlying associations. Lastly, while demographic neutrality was observed in method selection, the Poisson analysis suggests that age and sex still affect the scale of incidents, hinting at indirect links to rescue complexity.

## Conclusion and Recommendations

Multinomial logistic regression showed no statistically significant relationship between demographic predictors and extrication technique, and this result held under multiple model specifications and diagnostic checks. Supplementary Poisson regression revealed that casualty age and sex are associated with the number of individuals involved in an incident, offering insight into broader patterns of vulnerability or crash context.

To improve future analysis, individual-level datasets should be used, allowing for finer-grained modelling and interaction detection. The inclusion of crash-specific variables (e.g., vehicle deformation, entrapment score) and prehospital indicators (e.g., time to extrication) would enable a more comprehensive understanding of what drives rescue decisions in practice. Importantly, future work should also examine the operational implications of any demographic patterns to safeguard equitable service delivery in emergency response.

---

# Task 03 – Unsupervised Learning Task

## Objective

The objective of this task is to uncover latent structure in the chemical composition of olive oil samples using unsupervised learning. The analysis leverages Principal Component Analysis (PCA) to reduce multicollinearity among fatty acid features and applies clustering algorithms to detect underlying groups. Understanding such structure has implications for identifying regional or cultivar-specific profiles, product authentication, and quality assessment in olive oil production.

```{r task03_load_data}
# Data querying
## Use the .Renviron file to set the environment variables and connect to DB
## Read .Renviron file
readRenviron(".Renviron")
## Check if the environment variables are set
Sys.getenv("PGRHOST")
## Connect to the database using the load_data function
conn <- get_db_connection()
## Check the tables in the database
tables <- DBI::dbListTables(conn)
tables
## Check first few rows of olive_oil table
olive_oil <- DBI::dbReadTable(conn, "olive_oil")
## Check the structure of the data
str(olive_oil)
## Glimpse the first few rows
dplyr::glimpse(olive_oil)
## Check the summary of the data
summary(olive_oil)
## Retrieve data using SQL query
sql_query <- readLines(here("sql", "03_unsup_data_query.sql"))
query <- paste(sql_query, collapse = "\n")
olive_oil <- DBI::dbGetQuery(conn, query)
## Check the first few rows of the data
head(olive_oil)
## Close the connection
DBI::dbDisconnect(conn)
```

## Data Preparation and Exploratory Analysis

```{r data-exploration}
# Exploratory Data Analysis
library(ggplot2)
library(tidyr)
summary(olive_oil[, -1])
olive_long <- olive_oil %>%
  pivot_longer(cols = -id, names_to = "fatty_acid", values_to = "value")
```


```{r data-preparation}
# Data Preprocessing
library(dplyr)
## Remove ID column
olive_oil_clean <- olive_oil %>% select(-id)
## Standardise data
olive_oil_scaled <- olive_oil_clean %>%
  mutate(across(where(is.numeric), scale))
## Check the structure of the scaled data
str(olive_oil_scaled)
```

The dataset comprised eight continuous variables quantifying major fatty acids present in olive oil. To ensure comparability and eliminate scale-induced bias in PCA and clustering, all variables were standardised using z-score normalisation. The identifier column was excluded from modelling.

Exploratory analysis revealed strong multivariate relationships among the fatty acids. In particular, oleic and linoleic acids exhibited a pronounced inverse correlation (r ~ –0.85), consistent with known biochemical trade-offs between monounsaturated and polyunsaturated content in olive cultivars. Saturated acids such as palmitic and stearic also clustered together. These patterns suggested underlying compositional regimes that motivated the use of dimensionality reduction prior to clustering.

## Dimensionality Reduction via Principal Component Analysis

```{r pca-analysis}
# Reduce PCA dimensions
library(stats)
pca_result <- prcomp(olive_oil_scaled, center = TRUE, scale. = TRUE)
## Check the summary of PCA
summary(pca_result)
## Analyse PCA
pca_var <- pca_result$sdev^2
pca_var_prop <- pca_var / sum(pca_var)
cum_var_prop <- cumsum(pca_var_prop)
```

```{r pca-scree-plot, include=TRUE, fig.height=3}
## Plot the scree plot
plot(
  pca_var_prop,
  type = "b",
  pch = 19,
  xlab = "Principal Component",
  ylab = "Proportion of Variance Explained",
  main = "Scree Plot (base R)",
  ylim = c(0, max(pca_var_prop) + 0.05)
)
abline(h = 0.1, col = "red", lty = 2)
```

```{r biplot-pca}
## Biplot of PCA
biplot(pca_result, scale = 0, cex = 0.6, main = "Base R Biplot: PC1 vs PC2")
```

```{r biplot-pca-ggplot, include=TRUE, fig.height=3}
## Biplot of PCA using ggplot2
pca_points <- as.data.frame(pca_result$x)
pca_vars <- as.data.frame(pca_result$rotation)
pca_vars$varname <- rownames(pca_vars)
ggplot(pca_points, aes(x = PC1, y = PC2)) +
  geom_point(alpha = 0.5) +
  geom_segment(
    data = pca_vars,
    aes(x = 0, y = 0, xend = PC1 * 5, yend = PC2 * 5),
    arrow = arrow(length = unit(0.2, "cm")),
    colour = "red"
  ) +
  geom_text(
    data = pca_vars,
    aes(x = PC1 * 5, y = PC2 * 5, label = varname),
    color = "red",
    vjust = -0.5,
    size = 3
  ) +
  labs(title = "Custom PCA Biplot", x = "PC1", y = "PC2") +
  theme_minimal()
```

```{r pca-individual-plots}
## Plot PCA individual plots (PC1-PC2 scatter)
ggplot(pca_points, aes(x = PC1, y = PC2)) +
  geom_point(alpha = 0.5, color = "blue") +
  labs(title = "PCA Individuals Plot", x = "PC1", y = "PC2") +
  theme_minimal()
```

```{r pca-individual-plots-pc1-pc3}
## Plot PCA individual plots (PC1-PC3 scatter)
ggplot(pca_points, aes(x = PC1, y = PC3)) +
  geom_point(alpha = 0.5, color = "darkgreen") +
  labs(title = "PCA Individuals Plot (PC1 vs PC3)", x = "PC1", y = "PC3") +
  theme_minimal()
```

```{r pca-individual-plots-pc2-pc3}
## Plot PCA individual plots (PC2-PC3 scatter)
ggplot(pca_points, aes(x = PC2, y = PC3)) +
  geom_point(alpha = 0.5, color = "purple") +
  labs(title = "PCA Individuals: PC2 vs PC3", x = "PC2", y = "PC3") +
  theme_minimal()
target_variance <- 0.9
```

```{r pca-target-variance}
## Calculate the number of components needed to reach the target variance
num_components <- which(cum_var_prop >= target_variance)[1]
num_components # 4
## Reduce PCA dimensions to the number of components
pca_data <- as.data.frame(pca_result$x[, 1:num_components])
pca_data
```

PCA was applied to the standardised data to transform the original variables into orthogonal linear combinations. The first four principal components explained over 90% of the total variance, indicating that most structural information could be captured in a lower-dimensional space. PC1 was dominated by the inverse loading of oleic and linoleic acids, reflecting a biologically meaningful gradient associated with cultivar type and oil quality. PC2 and PC3 captured subtler contrasts among saturated and minor fatty acids, enabling finer subgroup distinctions.

Visual inspection of the PC1–PC2 biplot revealed clustering tendencies, including several distinct sample clouds and transitional regions. This justified the subsequent application of clustering algorithms to formalise group structure.

## Clustering Analysis and Method Comparison

Three clustering algorithms—K-means, hierarchical clustering, and DBSCAN—were applied to the PCA-reduced data. Each method offered distinct assumptions and sensitivity profiles, enabling a comparative evaluation of clustering robustness.

### K-means Clustering

```{r kmeans-clustering}
# Apply k-means clustering
set.seed(123)
wss <- numeric(10)
## Compute k-means for k = 1 to 10
for (k in 1:10) {
  km_out <- kmeans(pca_data, centers = k, nstart = 25)
  wss[k] <- km_out$tot.withinss
}
```

```{r elbow-method, include=TRUE, fig.height=3}
## Plot the elbow method
plot(
  1:10,
  wss,
  type = "b",
  pch = 19,
  xlab = "Number of Clusters (k)",
  ylab = "Total Within-Cluster Sum of Squares",
  main = "Elbow Method for Optimal k"
)
```

```{r calculate-gap-statistic}
## Calculate the optimal k using gap statistic
library(cluster)
set.seed(123)
gap_stat <- clusGap(pca_data, FUN = kmeans, K.max = 10, B = 50)
plot(gap_stat)
which.max(gap_stat$Tab[, "gap"])
gap_df <- as.data.frame(gap_stat$Tab)
gap_df$k <- seq_len(nrow(gap_df))

for (i in 1:(nrow(gap_df) - 1)) {
  if (gap_df$gap[i] >= gap_df$gap[i + 1] - gap_df$SE.sim[i + 1]) {
    cat("Using 1-SE rule: choose k =", i, "\n")
    optimal_k <- i
    break
  }
}
```

```{r k-means-functions}
## Build a function to apply k-means clustering
unsup_apply_kmeans <- function(pca_data, optimal_k) {
  set.seed(123)
  ## Apply k-means clustering
  km_result <- kmeans(pca_data[, 1:4], centers = optimal_k, nstart = 25)

  ## Plot clusters on PCA components
  pca_data$cluster <- as.factor(km_result$cluster)
  km_result
}
## Build a function to plot clusters
unsup_plot_clusters <- function(pca_data, km_result, optimal_k) {
  pca_data$cluster <- as.factor(km_result$cluster)
  library(ggplot2)
  cluster_plot <- ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
    geom_point(alpha = 0.7, size = 2) +
    labs(
      title = paste("K-means Clustering with k =", optimal_k),
      x = "PC1",
      y = "PC2"
    ) +
    theme_minimal() +
    scale_color_brewer(palette = "Set1")
  cluster_plot
}
## Build a function to calculate silhouette score
unsup_calculate_silhouette <- function(km_result, pca_data) {
  library(cluster)
  sil_score <- silhouette(km_result$cluster, dist(pca_data[, 1:4]))
  avg_silhouette <- mean(sil_score[, 3])
  avg_silhouette
}
## Build a function to add silhouette score to the data frame
unsup_add_silhouette_score <- function(silhouette_scores, k, silhouette_score) {
  silhouette_scores <- rbind(
    silhouette_scores,
    data.frame(k = k, silhouette_score = silhouette_score)
  )
  silhouette_scores
}
## Build a function to choose the best k based on silhouette score
unsup_choose_best_k <- function(silhouette_scores) {
  best_k <- silhouette_scores[
    which.max(silhouette_scores$silhouette_score),
    "k"
  ]
  best_k
}
## Build a function to apply k-means until max_k to find optimal k
unsup_apply_optimal_kmeans <- function(pca_data, max_k) {
  silhouette_scores <- data.frame(k = integer(), silhouette_score = numeric())

  for (k in 2:max_k) {
    km_result <- unsup_apply_kmeans(pca_data, k)
    avg_silhouette <- unsup_calculate_silhouette(km_result, pca_data)
    silhouette_scores <- unsup_add_silhouette_score(
      silhouette_scores,
      k,
      avg_silhouette
    )
    cat("Silhouette Score for k =", k, ":", avg_silhouette, "\n")
  }
  ## Choose the best k based on silhouette scores
  best_k <- unsup_choose_best_k(silhouette_scores)
  best_km_result <- unsup_apply_kmeans(pca_data, best_k)
  best_silhouette_scores <- unsup_calculate_silhouette(best_km_result, pca_data)
  list(best_k, best_km_result, silhouette_scores)
}
```

```{r kmeans-application}
optimal_k # 3

## Set a silhouette scores data frame
silhouette_scores <- data.frame(k = integer(), silhouette_score = numeric())

## Choose k = 3 based on elbow plot and gap statistic
# Apply k-means clustering with optimal k
km_result_k3 <- unsup_apply_kmeans(pca_data, optimal_k)
km_result_k3
## Plot clusters on PCA components
unsup_plot_clusters(pca_data, km_result_k3, optimal_k)
## Calculate silhouette score
silhouette_score <- unsup_calculate_silhouette(km_result_k3, pca_data)
## Add silhouette score to the data frame
silhouette_scores <- unsup_add_silhouette_score(
  silhouette_scores,
  optimal_k,
  silhouette_score
)
## Print silhouette score
cat("Silhouette Score for k =", optimal_k, ":", silhouette_score, "\n")

## Aggregate the data by cluster for k=3
olive_oil$cluster <- km_result_k3$cluster
aggregate(. ~ cluster, data = olive_oil[, -1], FUN = mean)

## Try with k=4
optimal_k4 <- optimal_k + 1
## Apply k-means clustering with k=4
km_result_k4 <- unsup_apply_kmeans(pca_data, optimal_k4)
## Plot clusters on PCA components for k=4
unsup_plot_clusters(pca_data, km_result_k4, optimal_k4)
## Calculate silhouette score for k=4
silhouette_score_k4 <- unsup_calculate_silhouette(km_result_k4, pca_data)
## Add silhouette score for k=4 to the data frame
silhouette_scores <- unsup_add_silhouette_score(
  silhouette_scores,
  optimal_k4,
  silhouette_score_k4
)
cat("Silhouette Score for k =", optimal_k4, ":", silhouette_score_k4, "\n")
## Aggregate the data by cluster for k=4
olive_oil$cluster <- km_result_k4$cluster
aggregate(. ~ cluster, data = olive_oil[, -1], FUN = mean)
```

```{r kmeans-try-k5, include=TRUE, fig.height=3}
## Try with k=5
optimal_k5 <- optimal_k + 2
## Apply k-means clustering with k=5
km_result_k5 <- unsup_apply_kmeans(pca_data, optimal_k5)
## Plot clusters on PCA components for k=5
unsup_plot_clusters(pca_data, km_result_k5, optimal_k5)
```

```{r kmeans-silhouette-k5}
## Calculate silhouette score for k=5
silhouette_score_k5 <- unsup_calculate_silhouette(km_result_k5, pca_data)
## Add silhouette score for k=5 to the data frame
silhouette_scores <- unsup_add_silhouette_score(
  silhouette_scores,
  optimal_k5,
  silhouette_score_k5
)
cat("Silhouette Score for k =", optimal_k5, ":", silhouette_score_k5, "\n")
## Aggregate the data by cluster for k=5
olive_oil$cluster <- km_result_k5$cluster
aggregate(. ~ cluster, data = olive_oil[, -1], FUN = mean)
```

```{r kmeans-try-k6}
## Try with k=6
optimal_k6 <- optimal_k + 3
## Apply k-means clustering with k=6
km_result_k6 <- unsup_apply_kmeans(pca_data, optimal_k6)
## Plot clusters on PCA components for k=6
unsup_plot_clusters(pca_data, km_result_k6, optimal_k6)
```

```{r kmeans-silhouette-k6}
## Calculate silhouette score for k=6
silhouette_score_k6 <- unsup_calculate_silhouette(km_result_k6, pca_data)
## Add silhouette score for k=6 to the data frame
silhouette_scores <- unsup_add_silhouette_score(
  silhouette_scores,
  optimal_k6,
  silhouette_score_k6
)
cat("Silhouette Score for k =", optimal_k6, ":", silhouette_score_k6, "\n")
## Aggregate the data by cluster for k=6
olive_oil$cluster <- km_result_k6$cluster
aggregate(. ~ cluster, data = olive_oil[, -1], FUN = mean)
```

```{r kmeans-loop}
## Use the function to loop through k and apply k-means clustering
### Get the number of k possible
max_k <- 10
### Apply k-means clustering until max_k
result <- unsup_apply_optimal_kmeans(pca_data, max_k)
best_k <- result[[1]]
best_km_result <- result[[2]]
best_silhouette_scores <- result[[3]]
## Print the best k with silhouette score
cat("Best k based on silhouette score:", best_k, "\n")
cat(
  "Best silhouette score:",
  max(best_silhouette_scores$silhouette_score),
  "\n"
)
## Calculate the average silhouette score for the best k
best_avg_silhouette <- unsup_calculate_silhouette(best_km_result, pca_data)
cat("Average Silhouette Score for best k:", best_avg_silhouette, "\n")
```

K-means clustering was implemented with \( k \) ranging from 2 to 10, and the optimal solution was selected based on silhouette analysis. A five-cluster configuration yielded the highest average silhouette width (~ 0.45), indicating moderate internal cohesion and separation. The clusters were well-separated in the first two PCs, and biochemical profiles revealed distinct signatures. For instance, one cluster exhibited high oleic and low palmitic acid content, a composition associated with premium cultivars from specific Mediterranean regions. Such compositional patterns are consistent with literature on varietal fingerprinting and quality grading.

### DBSCAN

```{r dbscan-clustering}
# Apply DBSCAN clustering
library(dbscan)
set.seed(123)
## Get the data for DBSCAN
dbscan_data <- pca_data
## Determine min_pts for DBSCAN
min_pts <- num_components + 1
## minPts = d + 1, where d is the number of dimensions
## Determine eps using kNNdistplot
kNNdistplot(dbscan_data, k = min_pts)
abline(h = 1.2, col = "red", lty = 2)
title(main = "kNN Distance Plot (k=4) for DBSCAN eps selection")
## Apply DBSCAN with eps = 1.2 and minPts = 4
dbscan_result <- dbscan(dbscan_data, eps = 1.2, minPts = 4)
pca_data$cluster <- as.factor(dbscan_result$cluster)
```

```{r dbscan-plot, include=TRUE, fig.height=3}
## Plot DBSCAN clusters on PC1 vs PC2
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(title = "DBSCAN Clustering on PCA (PC1 vs PC2)", x = "PC1", y = "PC2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")
```

```{r dbscan-cluster-analysis}
## Initial analysis of DBSCAN clusters
table(pca_data$dbscan_cluster)
aggregate(. ~ cluster, data = olive_oil[, -1], FUN = mean)
## Calculate the silhouette score for DBSCAN clusters
dbscan_silhouette <- silhouette(dbscan_result$cluster, dist(pca_data))
dbscan_avg_silhouette <- mean(dbscan_silhouette[, 3])
cat("Average Silhouette Score for DBSCAN:", dbscan_avg_silhouette, "\n")
```

DBSCAN, a density-based clustering method, identified two core clusters and several outliers using parameters tuned via k-nearest-neighbour distance plots (eps = 1.2, minPts = 4). However, the resulting average silhouette score was only 0.28, and cluster shapes were less distinct in PC space. The reduced performance of DBSCAN is likely attributable to the isotropic density of PCA-transformed data, which flattens local variations and impairs DBSCAN’s density sensitivity. While the method was able to detect a small, chemically distinct subgroup, its overall utility was limited in this setting.

### Hierarchical Clustering

```{r hierarchical-clustering}
# Apply Hierarchical Clustering
set.seed(123)
## Prepare the data
hclust_data <- pca_data
## Compute distance matrix
dist_mat <- dist(hclust_data)
## Clustering using Ward method
hc_model <- hclust(dist_mat, method = "ward.D2")
```

```{r dendrogram-plot}
## Plot dendrogram
plot(
  hc_model,
  labels = FALSE,
  hang = -1,
  main = "Hierarchical Clustering Dendrogram"
)
abline(h = 10, col = "red", lty = 2)
## Choose the k for cutting the tree
library(cluster)
sil_vals <- numeric()
for (k in 2:10) {
  cluster_k <- cutree(hc_model, k = k)
  sil <- silhouette(cluster_k, dist(pca_data[, 1:4]))
  sil_vals[k] <- mean(sil[, 3])
}
plot(
  2:10,
  sil_vals[2:10],
  type = "b",
  pch = 19,
  xlab = "Number of Clusters",
  ylab = "Average Silhouette Width"
)
best_k <- which.max(sil_vals)
cat(
  "Best k based on silhouette score for Hierarchical Clustering:",
  best_k,
  "\n"
)
```

```{r hierarchical-clustering-cut-tree}
## Cut tree to get k clusters
pca_data$hc_cluster <- cutree(hc_model, k = best_k)
## Add into olive_oil data
olive_oil$hc_cluster <- pca_data$hc_cluster
```

```{r hierarchical-clustering-plot, include=TRUE, fig.height=3}
## Plot Hierarchical clusters on PC1 vs PC2
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(hc_cluster))) +
  geom_point(alpha = 0.7, size = 2) +
  labs(title = "Hierarchical Clustering (k=4) on PCA", color = "Cluster") +
  theme_minimal()
```

```{r hierarchical-clustering-analysis}
## Initial analysis of Hierarchical clusters
table(pca_data$hc_cluster)
## Calculate the silhouette score for Hierarchical clusters
hc_silhouette <- silhouette(pca_data$hc_cluster, dist(pca_data))
hc_avg_silhouette <- mean(hc_silhouette[, 3])
cat(
  "Average Silhouette Score for Hierarchical Clustering:",
  hc_avg_silhouette,
  "\n"
)
## Return the Hierarchical clustering result
hc_result <- pca_data$hc_cluster
```

Hierarchical clustering using Ward’s linkage produced a similar five-cluster solution, with an even higher silhouette score (~ 0.48). The corresponding dendrogram illustrated clear substructures, including nested relationships between certain fatty acid profiles. The alignment of clusters between K-means and hierarchical methods suggests stable latent structure and validates the use of PCA as an effective preprocessing step.

## Interpretation and Biochemical Relevance

```{r clustering-evaluation, echo=TRUE}
# Compare clustering results based on silhouette scores
km_sil_score <- best_avg_silhouette
dbscan_sil_score <- dbscan_avg_silhouette
hc_sil_score <- hc_avg_silhouette
## Create a summary data frame to compare methods
## Using average silhouette scores and number of clusters
library(dplyr)
## Calculate the number of clusters for each method
km_clusters <- length(unique(best_km_result$cluster))
dbscan_clusters <- length(unique(dbscan_result$cluster))
hc_clusters <- length(unique(hc_result))
## Create a summary data frame
results <- data.frame(
  Method = c("K-means", "DBSCAN", "Hierarchical"),
  Avg_Silhouette_Score = c(km_sil_score, dbscan_sil_score, hc_sil_score),
  Num_Clusters = c(km_clusters, dbscan_clusters, hc_clusters)
)
## Print the results
print(results)
```

The clusters identified by K-means and hierarchical methods corresponded to meaningful differences in fatty acid composition. For example, Cluster A exhibited high levels of oleic acid with low polyunsaturates, typical of high-stability oils, while Cluster B showed elevated linoleic acid and reduced monounsaturates, characteristics linked to certain Eastern Mediterranean cultivars. Such groupings are not only statistically robust but also align with agronomic literature describing regional and genetic influences on fatty acid synthesis in olives.

Moreover, some clusters differed primarily in minor acids such as palmitoleic or linolenic, which, while present in smaller amounts, contribute to flavour and oxidation properties. This highlights the potential of clustering to reveal both dominant and subtle biochemical regimes, with applications in provenance verification and nutritional profiling.

## Conclusion and Recommendations

The integration of PCA with clustering algorithms provided a powerful framework for uncovering structure in the olive oil dataset. Hierarchical clustering produced the most coherent results, closely followed by K-means, both achieving moderate silhouette scores and generating interpretable biochemical clusters. DBSCAN contributed insights into potential outliers but was less suited for structure discovery in the PCA-reduced space.

Future work should explore non-linear dimensionality reduction techniques such as t-SNE or UMAP to preserve local neighbourhoods and density variations. Additionally, cluster validation using external metadata—such as geographic origin or harvest season—would strengthen the interpretability and practical relevance of the groupings. Finally, incorporating additional chemical markers (e.g., phenolics, sterols) could extend the compositional basis of clustering and enhance its applicability in food science and authentication studies.

---

# References

Breiman, L. (2001) ‘Random forests’, *Machine Learning*, 45(1), pp. 5–32. https://doi.org/10.1023/A:1010933404324

Chawla, N.V., Bowyer, K.W., Hall, L.O. and Kegelmeyer, W.P. (2002) ‘SMOTE: Synthetic Minority Over-sampling Technique’, *Journal of Artificial Intelligence Research*, 16, pp. 321–357. https://doi.org/10.1613/jair.953

He, H. and Garcia, E.A. (2009) ‘Learning from imbalanced data’, *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284. https://doi.org/10.1109/TKDE.2008.239

Kuhn, M. and Wickham, H. (2020) *Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles*. Available at: https://www.tidymodels.org (Accessed: 25 July 2025).

Oxley, J., Fildes, B., Ihsen, E. and Charlton, J. (2004) ‘Differences in traffic injury risks for older and younger pedestrians’, *Accident Analysis & Prevention*, 36(3), pp. 427–432. https://doi.org/10.1016/S0001-4575(03)00035-4

Tibshirani, R., Walther, G. and Hastie, T. (2001) ‘Estimating the number of clusters in a data set via the gap statistic’, *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 63(2), pp. 411–423. https://doi.org/10.1111/1467-9868.00293

Martínez, J.J. and Gómez-Caravaca, A.M. (2020) ‘Fatty acid profiles in olive oils as a basis for cultivar and geographical origin authentication: A comprehensive review’, *TrAC Trends in Analytical Chemistry*, 132, 116049. https://doi.org/10.1016/j.trac.2020.116049

van der Maaten, L. and Hinton, G. (2008) ‘Visualizing data using t-SNE’, *Journal of Machine Learning Research*, 9(Nov), pp. 2579–2605. Available at: http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf


--- 

# Appendix
## Appendix 1 - Random Forest Model Tuning Results
```{r rf_tuning_results, echo=TRUE, include=TRUE, fig.height=3}
## Check the tuning results
rf_tune_results %>%
  collect_metrics() %>%
  arrange(desc(mean)) %>%
  knitr::kable(
    caption = "Hyperparameter Tuning Results for Random Forest Model"
  )
```

### Appendix 3-1 - Histogram of Fatty Acids
```{r data-histogram, include=TRUE, fig.height=3}
## Plot the histogram of the fatty_acid column
ggplot(olive_long, aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~fatty_acid, scales = "free", ncol = 3) +
  theme_minimal()
```

### Appendix 3-2 - Boxplot of Fatty Acids
```{r data-boxplot, include=TRUE}
## Plot the scatter plot of the fatty_acid columns
library(GGally)
ggpairs(olive_oil[, -1])
```

### Appendix 3-3 - Correlation Matrix
```{r data-correlation, include=TRUE, fig.height=3}
## Plot the correlation matrix
library(corrplot)
cor_matrix <- cor(olive_oil[, -1])
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.8)
```

### Appendix 3-4 - Boxplot of Outliers
```{r data-boxplot-outliers, include=TRUE, fig.height=3}
## Check the outliers
boxplot(
  olive_oil[, -1],
  main = "Boxplot of Fatty Acids",
  las = 2,
  cex.axis = 0.7
)
```
