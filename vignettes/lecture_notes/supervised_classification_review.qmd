---
title: "Supervised classification"
author: "Paul Hewson"
format: revealjs
self-contained: true
editor: visual
---

## Reflecting on Supervised Classification

::: {style="font-size: 30px;"}
-   Chapter 11 applies a range of methods to the same dataset
-   In reality, different problems (data sets) suit different classifiers
-   You should build up your own summary of this
-   My very brief thoughts, as a starter, are included here. Things to think about include:
    -   What kind of data can a classifer deal with (quantitative/qualitative, missing)
    -   What data preparation is needed (scaling, selection)
    -   How important is train/test split
    -   Computational requirements (speed, memory)
:::

## Summary Table

::: {style="font-size: 25px;"}
| Method                     | Advantages                                              | Disadvantages                                                              |
|------------------|-----------------------|-------------------------------|
| **rpart** (Decision Trees) | Interpretable, Handles Non-Linearity, Feature Selection | Overfitting, Instability, Limited Performance                              |
| **Random Forests**         | High Accuracy, Handles Missing Data, Feature Importance | Computationally Expensive, Less Interpretability, High Memory Usage        |
| **kNN**                    | Simple, No Training Phase, Good for Small Data          | Slow for Large Data, Sensitive to Noise, Requires Scaling                  |
| **Na√Øve Bayes**            | Fast, Handles Missing Data, Good for Text               | Independence Assumption, Limited Expressiveness, Needs Well-Formatted Data |
| **Neural Networks**        | Handles Complex Patterns, Scalable, Feature Learning    | Computationally Intensive, Black Box, Needs Large Data                     |
:::

```{=html}
<!--

1.  rpart (Recursive Partitioning, Decision Trees) Implemented in R via the rpart package. 

Advantages: 
Interpretability ‚Äì Produces easy-to-understand decision trees. 
Handles Non-Linearity ‚Äì Can model complex relationships without requiring transformations. Feature Selection ‚Äì Automatically selects important features during tree construction. 

Disadvantages: 
Overfitting ‚Äì Prone to overfitting, especially with deep trees. 
Instability ‚Äì Small changes in data can lead to drastically different trees. 
Limited Performance ‚Äì Often less accurate than ensemble methods like Random Forests.

2.  Random Forests

Implemented via the randomForest package.

Advantages:
High Accuracy ‚Äì Reduces overfitting by averaging multiple trees.
Handles Missing Data ‚Äì Can handle missing values and categorical variables well.
Feature Importance ‚Äì Provides insights into feature importance.

Disadvantages:
Computationally Expensive ‚Äì Training can be slow for large datasets.
Less Interpretability ‚Äì Harder to understand compared to a single decision tree.
Memory Usage ‚Äì Requires more memory due to multiple trees.


3.  k-Nearest Neighbors (kNN)

Implemented via the class package. Advantages:

Simple & Intuitive ‚Äì Easy to understand and implement.
No Training Phase ‚Äì Just stores the dataset and classifies based on neighbors.
Works Well for Small Data ‚Äì Effective when the dataset is small and well-structured.


Disadvantages:

Computationally Expensive ‚Äì Slow for large datasets due to distance calculations.
Sensitive to Noise ‚Äì Outliers can significantly affect classification.
Feature Scaling Required ‚Äì Requires normalization for meaningful distance calculations.

4.  Na√Øve Bayes

Implemented via the e1071 package. Advantages:

Fast & Efficient ‚Äì Works well with large datasets.
Handles Missing Data ‚Äì Can handle missing values using probability estimates.
Works Well with Text Data ‚Äì Commonly used for spam filtering and sentiment analysis.

Disadvantages:
Strong Independence Assumption ‚Äì Assumes features are independent, which is often unrealistic.
Limited Expressiveness ‚Äì Struggles with complex relationships between features.
Requires Well-Formatted Data ‚Äì Performance drops if features are highly correlated.

5.  Neural Networks

Implemented via the nnet or keras package. Advantages:

Handles Complex Patterns ‚Äì Can model highly non-linear relationships.
Scalability ‚Äì Works well with large datasets.
Feature Learning ‚Äì Automatically extracts relevant features.

Disadvantages:

Computationally Intensive ‚Äì Requires significant processing power.
Black Box Model ‚Äì Hard to interpret compared to decision trees.
Requires Large Data ‚Äì Needs a lot of data to generalize well.
-->
```
## **Quick Selection Guide**

::: {style="font-size: 30px;"}
| **Scenario**                              | **Best Method**        |
|-------------------------------------------|------------------------|
| **Interpretable model**                   | rpart (Decision Trees) |
| **High accuracy & robustness**            | Random Forests         |
| **Small dataset & simple patterns**       | kNN                    |
| **Text classification (spam, sentiment)** | Na√Øve Bayes            |
| **Complex patterns & large data**         | Neural Networks        |
:::

```{=html}
<!--

1.  rpart (Decision Trees) Best for:

Interpretable models ‚Äì When you need a clear, explainable decision-making process. Small to medium datasets ‚Äì Works well when data isn‚Äôt too large. Feature selection ‚Äì Automatically identifies important variables. Avoid if:

You have high-dimensional data ‚Äì Trees may not perform well with many features. You need high accuracy ‚Äì Often outperformed by ensemble methods like Random Forests.

2.  Random Forests Best for:

High accuracy ‚Äì Works well for complex datasets with many features. Handling missing values ‚Äì Can deal with missing data better than many other methods. Feature importance analysis ‚Äì Helps identify which variables matter most. Avoid if:

You need interpretability ‚Äì Hard to explain individual predictions. You have limited computational resources ‚Äì Training can be slow for large datasets.

3.  k-Nearest Neighbors (kNN) Best for:

Small datasets ‚Äì Works well when data is not too large. Non-linear relationships ‚Äì Can capture complex patterns without needing a model. Instance-based learning ‚Äì No training phase, just stores the dataset.

Avoid if:

You have large datasets ‚Äì Slow due to distance calculations. You have high-dimensional data ‚Äì Performance drops when features increase. You need robustness to noise ‚Äì Sensitive to outliers.

4.  Na√Øve Bayes Best for:

Text classification ‚Äì Spam filtering, sentiment analysis, etc. Fast predictions ‚Äì Works well for real-time applications. Handling missing data ‚Äì Uses probability estimates to deal with missing values. Avoid if:

Your features are highly correlated ‚Äì Assumes independence, which may not hold. You need complex decision boundaries ‚Äì Struggles with intricate relationships.

5.  Neural Networks Best for:

Deep learning tasks ‚Äì Image recognition, speech processing, etc. Large datasets ‚Äì Performs well when trained on massive amounts of data. Complex relationships ‚Äì Can model highly non-linear patterns. Avoid if:

You need interpretability ‚Äì Hard to explain why a neural network makes a decision. You have limited data ‚Äì Needs a lot of examples to generalize well. You need fast training ‚Äì Computationally expensive.
-->
```
## Test / Train

::: {style="font-size: 30px;"}
| **Classifier**             | **Impact of Train/Test Split** | **Why?**                               |
|--------------------|-----------------------|-----------------------------|
| **rpart (Decision Trees)** | **High**                       | Overfits easily, needs pruning         |
| **Random Forests**         | **Moderate**                   | Reduces overfitting via averaging      |
| **kNN**                    | **Moderate**                   | Sensitive to data distribution         |
| **Na√Øve Bayes**            | **Low to Moderate**            | Works well with small data             |
| **Neural Networks**        | **Very High**                  | Needs large data, prone to overfitting |
:::

```{=html}
<!--

1.  rpart (Decision Trees)

Test/train split is crucial because decision trees tend to overfit if trained on the entire dataset. 
A small training set can lead to an unstable tree that doesn‚Äôt generalize well. 
Cross-validation is often used to prune the tree and improve generalization.

üîπ Impact: High ‚Äì Poor train/test splits can lead to overfitting or underfitting.

2. Random Forests

Less sensitive to train/test splits because it averages multiple trees. 
Overfitting is reduced due to bootstrapping (sampling with replacement). 
Still needs a good split to ensure diverse training samples.

üîπ Impact: Moderate ‚Äì Less prone to overfitting than rpart, but still benefits from a good split. 

3.  k-Nearest Neighbors (kNN)

Test/train split matters, but not as much because kNN doesn‚Äôt "train" in the traditional sense. 
Performance depends on data distribution ‚Äì If the test set is very different from the training set, accuracy drops. 
Stratified sampling is recommended to ensure balanced class representation.

Impact: Moderate ‚Äì Sensitive to data distribution but doesn‚Äôt overfit like trees. 

4. Na√Øve Bayes

Less dependent on train/test splits because it relies on probability estimates. 
Works well even with small training sets due to its strong assumptions.
Class imbalance can affect probability calculations, so a good split still helps.

Impact: Low to Moderate ‚Äì Works well with small data but benefits from balanced splits. 

5. Neural Networks

Test/train split is extremely important due to high risk of overfitting.
Needs large training data to generalize well. Cross-validation and regularization (dropout, L2 penalty) help mitigate overfitting.

Impact: Very High ‚Äì Poor splits can lead to extreme overfitting or poor generalization.

-->
```
## Inbalance

::: {style="font-size: 30px;"}
| **Method**                       | **Best for**                                    | **Potential Downsides**   |
|--------------------|----------------------------|------------------------|
| **Oversampling (SMOTE)**         | Small datasets, minority class underrepresented | Can introduce noise       |
| **Undersampling**                | Large datasets, majority class dominates        | Loss of information       |
| **Class Weights**                | Models like Random Forest, SVM, Neural Networks | Requires tuning           |
| **Boosting (XGBoost, AdaBoost)** | High-performance models                         | Computationally expensive |
:::
